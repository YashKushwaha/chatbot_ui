{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c66226-07bd-475f-8810-c4ad8cd7a0a6",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/memory/ChatSummaryMemoryBuffer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faeac97c-67bc-4d99-b8f3-156049c888e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatSummaryMemoryBuffer\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6732b10f-93f7-40e8-b505-f5637e8f9e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BaseMemory',\n",
       " 'BaseMemoryBlock',\n",
       " 'ChatMemoryBuffer',\n",
       " 'ChatSummaryMemoryBuffer',\n",
       " 'FactExtractionMemoryBlock',\n",
       " 'InsertMethod',\n",
       " 'Memory',\n",
       " 'SimpleComposableMemory',\n",
       " 'StaticMemoryBlock',\n",
       " 'VectorMemory',\n",
       " 'VectorMemoryBlock',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'chat_memory_buffer',\n",
       " 'chat_summary_memory_buffer',\n",
       " 'memory',\n",
       " 'memory_blocks',\n",
       " 'simple_composable_memory',\n",
       " 'types',\n",
       " 'vector_memory']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index.core.memory  as m\n",
    "dir(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e45b6ae-4611-4e8a-a9a6-d510ef993f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"qwen3:14b\"\n",
    "model = 'qwen3:14b'\n",
    "tokenizer_model = \"Qwen/Qwen3-14B\"\n",
    "context_window = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "653b40c9-be54-47a9-85c3-47c70ab76a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model=model,\n",
    "    request_timeout=120.0,\n",
    "    thinking=True,\n",
    "    context_window=context_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00dcd434-a73b-4ee1-bd8a-dfe0bfa412b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"Hello! I'm just a chatbot, so I don't have feelings, but I'm here and ready to chat! ðŸ˜Š How are you today? Let me know if there's anything I can help with!\", additional_kwargs={'tool_calls': None, 'thinking': 'Okay, the user greeted me with \"Hi, how are you?\" I need to respond appropriately. Since I\\'m an AI, I don\\'t have feelings, but I should acknowledge their greeting warmly. I should keep the response friendly and open-ended to encourage further conversation. Maybe ask them how they\\'re doing and offer help. Let me check the previous examples to ensure consistency. Yep, that\\'s right. Make sure to use an emoji to keep it approachable. Alright, time to put it all together.\\n'}, raw={'model': 'qwen3:14b', 'created_at': '2025-06-21T18:16:06.720886073Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10741171427, 'load_duration': 5876469963, 'prompt_eval_count': 16, 'prompt_eval_duration': 222075134, 'eval_count': 151, 'eval_duration': 4641725696, 'message': Message(role='assistant', content=\"Hello! I'm just a chatbot, so I don't have feelings, but I'm here and ready to chat! ðŸ˜Š How are you today? Let me know if there's anything I can help with!\", thinking='Okay, the user greeted me with \"Hi, how are you?\" I need to respond appropriately. Since I\\'m an AI, I don\\'t have feelings, but I should acknowledge their greeting warmly. I should keep the response friendly and open-ended to encourage further conversation. Maybe ask them how they\\'re doing and offer help. Let me check the previous examples to ensure consistency. Yep, that\\'s right. Make sure to use an emoji to keep it approachable. Alright, time to put it all together.\\n', images=None, tool_calls=None), 'usage': {'prompt_tokens': 16, 'completion_tokens': 151, 'total_tokens': 167}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.complete('Hi, how are you ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dff92f95-f53d-4ae0-8fdf-305619e5f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    ChatMessage(role=\"user\", content=\"What is LlamaIndex?\"),\n",
    "    ChatMessage(\n",
    "        role=\"assistant\",\n",
    "        content=\"LlamaaIndex is the leading data framework for building LLM applications\",\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Can you give me some more details?\"),\n",
    "    ChatMessage(\n",
    "        role=\"assistant\",\n",
    "        content=\"\"\"LlamaIndex is a framework for building context-augmented LLM applications. Context augmentation refers to any use case that applies LLMs on top of your private or domain-specific data. Some popular use cases include the following: \n",
    "        Question-Answering Chatbots (commonly referred to as RAG systems, which stands for \"Retrieval-Augmented Generation\"), Document Understanding and Extraction, Autonomous Agents that can perform research and take actions\n",
    "        LlamaIndex provides the tools to build any of these above use cases from prototype to production. The tools allow you to both ingest/process this data and implement complex query workflows combining data access with LLM prompting.\"\"\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08539941-c0f0-40bc-b1e0-6536a529728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fcf6768-4e21-4483-ab3a-25302d60b8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13048, 11, 1096, 374, 264, 11652]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Hi, This is a sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cebf09cf-7c23-472a-bfd4-31a8d05cccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(13048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9264d87-67b6-4f98-be73-ace5b995e9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 |\n",
      "8948 system\n",
      "91 |\n",
      "25 :\n",
      "1446  You\n",
      "525  are\n",
      "264  a\n",
      "220  \n",
      "10950  helpful\n",
      "17847  assistant\n"
     ]
    }
   ],
   "source": [
    "prompt = \"|system|: You are a  helpful assistant\"\n",
    "for i in tokenizer.encode(prompt):\n",
    "    print(i, tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa5786a0-f507-4f40-8de8-7ba6dd41d32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644,\n",
       " 872,\n",
       " 198,\n",
       " 3838,\n",
       " 374,\n",
       " 279,\n",
       " 6722,\n",
       " 315,\n",
       " 9625,\n",
       " 30,\n",
       " 151645,\n",
       " 198,\n",
       " 151644,\n",
       " 77091,\n",
       " 198]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": 'What is the capital of France?'}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fad402b9-aa1f-4a4e-8aaa-9b56cfb47b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151644 <|im_start|>\n",
      "872 user\n",
      "198 \n",
      "\n",
      "3838 What\n",
      "374  is\n",
      "279  the\n",
      "6722  capital\n",
      "315  of\n",
      "9625  France\n",
      "30 ?\n",
      "151645 <|im_end|>\n",
      "198 \n",
      "\n",
      "151644 <|im_start|>\n",
      "77091 assistant\n",
      "198 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in text:\n",
    "    print(i, tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e82afe81-62d3-4ae4-8c31-d1f7407c385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_llm = llm\n",
    "#tokenizer_fn = tiktoken.encoding_for_model(model).encode\n",
    "tokenizer_fn = lambda text: tokenizer.encode(text, add_special_tokens=False)\n",
    "memory = ChatSummaryMemoryBuffer.from_defaults(\n",
    "    chat_history=chat_history,\n",
    "    llm=summarizer_llm,\n",
    "    token_limit=2,\n",
    "    tokenizer_fn=tokenizer_fn,\n",
    ")\n",
    "\n",
    "history = memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d52d626d-e15d-4afd-890f-38d8190de658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChatSummaryMemoryBuffer in module llama_index.core.memory.chat_summary_memory_buffer:\n",
      "\n",
      "class ChatSummaryMemoryBuffer(llama_index.core.memory.types.BaseMemory)\n",
      " |  ChatSummaryMemoryBuffer(*, token_limit: int, count_initial_tokens: bool = False, llm: Optional[Annotated[llama_index.core.llms.llm.LLM, SerializeAsAny()]] = None, summarize_prompt: Optional[str] = None, tokenizer_fn: Callable[[str], List] = <factory>, chat_store: typing.Annotated[llama_index.core.storage.chat_store.base.BaseChatStore, SerializeAsAny()] = <factory>, chat_store_key: str = 'chat_history') -> None\n",
      " |  \n",
      " |  Deprecated: Please use `llama_index.core.memory.Memory` instead.\n",
      " |  \n",
      " |  Buffer for storing chat history that uses the full text for the latest\n",
      " |  {token_limit}.\n",
      " |  \n",
      " |  All older messages are iteratively summarized using the {llm} provided, with\n",
      " |  the max number of tokens defined by the {llm}.\n",
      " |  \n",
      " |  User can specify whether initial tokens (usually a system prompt)\n",
      " |  should be counted as part of the {token_limit}\n",
      " |  using the parameter {count_initial_tokens}.\n",
      " |  \n",
      " |  This buffer is useful to retain the most important information from a\n",
      " |  long chat history, while limiting the token count and latency\n",
      " |  of each request to the LLM.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ChatSummaryMemoryBuffer\n",
      " |      llama_index.core.memory.types.BaseMemory\n",
      " |      llama_index.core.schema.BaseComponent\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  async aput(self, message: llama_index.core.base.llms.types.ChatMessage) -> None\n",
      " |      Put chat history.\n",
      " |  \n",
      " |  get(self, input: Optional[str] = None, initial_token_count: int = 0, **kwargs: Any) -> List[llama_index.core.base.llms.types.ChatMessage]\n",
      " |      Get chat history.\n",
      " |  \n",
      " |  get_all(self) -> List[llama_index.core.base.llms.types.ChatMessage]\n",
      " |      Get all chat history.\n",
      " |  \n",
      " |  get_token_count(self) -> int\n",
      " |      Returns the token count of the memory buffer (excluding the last assistant response).\n",
      " |  \n",
      " |  model_post_init = init_private_attributes(self: 'BaseModel', context: 'Any', /) -> 'None'\n",
      " |      This function is meant to behave like a BaseModel method to initialise private attributes.\n",
      " |      \n",
      " |      It takes context as an argument since that's what pydantic-core passes when calling it.\n",
      " |      \n",
      " |      Args:\n",
      " |          self: The BaseModel instance.\n",
      " |          context: The context.\n",
      " |  \n",
      " |  put(self, message: llama_index.core.base.llms.types.ChatMessage) -> None\n",
      " |      Put chat history.\n",
      " |  \n",
      " |  reset(self) -> None\n",
      " |      Reset chat history.\n",
      " |  \n",
      " |  serialize_courses_in_order(self, chat_store: llama_index.core.storage.chat_store.base.BaseChatStore) -> dict\n",
      " |  \n",
      " |  set(self, messages: List[llama_index.core.base.llms.types.ChatMessage]) -> None\n",
      " |      Set chat history.\n",
      " |  \n",
      " |  to_dict(self, **kwargs: Any) -> dict\n",
      " |      Convert memory to dict.\n",
      " |  \n",
      " |  to_string(self) -> str\n",
      " |      Convert memory to string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  class_name() -> str from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Get class name.\n",
      " |  \n",
      " |  from_defaults(chat_history: Optional[List[llama_index.core.base.llms.types.ChatMessage]] = None, llm: Optional[llama_index.core.llms.llm.LLM] = None, chat_store: Optional[llama_index.core.storage.chat_store.base.BaseChatStore] = None, chat_store_key: str = 'chat_history', token_limit: Optional[int] = None, tokenizer_fn: Optional[Callable[[str], List]] = None, summarize_prompt: Optional[str] = None, count_initial_tokens: bool = False, **kwargs: Any) -> 'ChatSummaryMemoryBuffer' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Create a chat memory buffer from an LLM\n",
      " |      and an initial list of chat history messages.\n",
      " |  \n",
      " |  from_dict(data: Dict[str, Any], **kwargs: Any) -> 'ChatSummaryMemoryBuffer' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  from_string(json_str: str, **kwargs: Any) -> 'ChatSummaryMemoryBuffer' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Create a chat memory buffer from a string.\n",
      " |  \n",
      " |  validate_memory(values: dict) -> dict from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate the memory.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_token_count': <class 'int'>, 'chat_store': typing...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __private_attributes__ = {'_token_count': ModelPrivateAttr(default=0)}\n",
      " |  \n",
      " |  __pydantic_complete__ = True\n",
      " |  \n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |  \n",
      " |  __pydantic_core_schema__ = {'definitions': [{'function': {'function': ...\n",
      " |  \n",
      " |  __pydantic_custom_init__ = False\n",
      " |  \n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |  \n",
      " |  __pydantic_fields__ = {'chat_store': FieldInfo(annotation=BaseChatStor...\n",
      " |  \n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |  \n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |  \n",
      " |  __pydantic_post_init__ = 'model_post_init'\n",
      " |  \n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=FunctionWrap(\n",
      " |    ...\n",
      " |  \n",
      " |  __pydantic_setattr_handlers__ = {'_token_count': <function _private_se...\n",
      " |  \n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"ChatSummaryMemoryBuffe...\n",
      " |  \n",
      " |  __signature__ = <Signature (*, token_limit: int, count_initial_t...>, ...\n",
      " |  \n",
      " |  model_config = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.core.memory.types.BaseMemory:\n",
      " |  \n",
      " |  async aget(self, input: Optional[str] = None, **kwargs: Any) -> List[llama_index.core.base.llms.types.ChatMessage]\n",
      " |      Get chat history.\n",
      " |  \n",
      " |  async aget_all(self) -> List[llama_index.core.base.llms.types.ChatMessage]\n",
      " |      Get all chat history.\n",
      " |  \n",
      " |  async aput_messages(self, messages: List[llama_index.core.base.llms.types.ChatMessage]) -> None\n",
      " |      Put chat history.\n",
      " |  \n",
      " |  async areset(self) -> None\n",
      " |      Reset chat history.\n",
      " |  \n",
      " |  async aset(self, messages: List[llama_index.core.base.llms.types.ChatMessage]) -> None\n",
      " |      Set chat history.\n",
      " |  \n",
      " |  put_messages(self, messages: List[llama_index.core.base.llms.types.ChatMessage]) -> None\n",
      " |      Put chat history.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.core.schema.BaseComponent:\n",
      " |  \n",
      " |  __getstate__(self) -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  __setstate__(self, state: 'Dict[str, Any]') -> 'None'\n",
      " |  \n",
      " |  custom_model_dump(self, handler: 'SerializerFunctionWrapHandler', info: 'SerializationInfo') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  dict(self, **kwargs: 'Any') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  json(self, **kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  to_json(self, **kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from llama_index.core.schema.BaseComponent:\n",
      " |  \n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler') -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |  \n",
      " |  from_json(data_str: 'str', **kwargs: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from llama_index.core.schema.BaseComponent:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |  \n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |  \n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |  \n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |      \n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |  \n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |  \n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'str'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      " |      Returns the string representation of a recursive object.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |      \n",
      " |      If you need `include` or `exclude`, use:\n",
      " |      \n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |  \n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/serialization.md#model_copy)\n",
      " |      \n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |      \n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |  \n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#modelmodel_dump)\n",
      " |      \n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |  \n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#modelmodel_dump_json)\n",
      " |      \n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |      \n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |      \n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |  \n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |      \n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      \n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |  \n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Generates a JSON schema for a model class.\n",
      " |      \n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |  \n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |      \n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |      \n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |  \n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |      \n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |      \n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |  \n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate a pydantic model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |  \n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |      \n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |  \n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |  \n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |  \n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __pydantic_extra__\n",
      " |  \n",
      " |  __pydantic_fields_set__\n",
      " |  \n",
      " |  __pydantic_private__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __pydantic_root_model__ = False\n",
      " |  \n",
      " |  model_computed_fields = {}\n",
      " |  \n",
      " |  model_fields = {'chat_store': FieldInfo(annotation=BaseChatStore, requ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ChatSummaryMemoryBuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33404175-f7f9-4abf-bb9f-af3bbca95fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='The conversation explains that LlamaIndex is a framework for building context-augmented LLM applications, enabling the use of private or domain-specific data with LLMs. Key use cases include Retrieval-Augmented Generation (RAG) systems, document understanding, and autonomous agents. The framework provides tools for data ingestion, processing, and complex query workflows integrating data access with LLM prompting.')])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ccf5f-1016-4ffb-9c93-468cbbf6e967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
