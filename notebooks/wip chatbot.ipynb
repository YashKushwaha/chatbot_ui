{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2826b52a-5c93-4378-a4e3-3d8d81d57b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import Memory\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8800748-ae9b-4255-a29c-816c42ee4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"qwen3:14b\"\n",
    "model = 'qwen3:14b'\n",
    "tokenizer_model = \"Qwen/Qwen3-14B\"\n",
    "context_window = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b91c55-916d-480d-98dd-8c54a5d0c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model=model,\n",
    "    request_timeout=120.0,\n",
    "    thinking=True,\n",
    "    context_window=context_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2271b865-7aa8-4a48-8d7c-33972ebf350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b49354e-4c5c-4f66-b2d3-0e61eb5d5f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SimpleChatEngine.__init__() missing 3 required positional arguments: 'llm', 'memory', and 'prefix_messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chat_engine \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleChatEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: SimpleChatEngine.__init__() missing 3 required positional arguments: 'llm', 'memory', and 'prefix_messages'"
     ]
    }
   ],
   "source": [
    "# Intialize without anything to see what are the mandatory inputs for creating chat engine\n",
    "chat_engine = SimpleChatEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4716dd61-4b54-4ea9-8a1a-721ac7e0726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASSISTANT', 'CHATBOT', 'DEVELOPER', 'FUNCTION', 'MODEL', 'SYSTEM', 'TOOL', 'USER', '__class__', '__doc__', '__members__', '__module__']\n"
     ]
    }
   ],
   "source": [
    "print(dir(MessageRole))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1769c805-db7f-4ef4-8201-9c10963ed5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory.from_defaults(token_limit=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef2de0c7-c8f9-4d12-8fe2-76ae24ea0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = ChatMessage(content=\"You are an expert system.\", role=MessageRole.SYSTEM)\n",
    "prefix_messages = [system_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee9acecb-5495-4a3a-a994-f33b36aeced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = SimpleChatEngine(llm=llm, memory=memory, prefix_messages=prefix_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9200b5b-4edb-4954-921d-3867b1f18e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response=\"Hello! I'm Qwen, a large language model developed by Alibaba Cloud. I can help you with a wide range of tasks, such as answering questions, creating content, and having conversations. How can I assist you today?\", sources=[], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = chat_engine.chat(message='Hi, What is your name ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "355bdbe8-63b1-4dee-a1cf-07695b93ee48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Hi, What is your name ?')]),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': None, 'thinking': 'Okay, the user asked, \"Hi, What is your name?\" I need to respond appropriately. Let me think about how to approach this.\\n\\nFirst, the user is introducing themselves and asking for my name. Since I\\'m an AI, I should mention my name and maybe a bit about my purpose. But I should keep it friendly and concise.\\n\\nI should start with a greeting to match their \"Hi.\" Then state my name, which is Qwen. Next, explain that I\\'m a large language model developed by Alibaba Cloud. It\\'s good to mention my capabilities like answering questions, creating content, and having conversations. Also, include that I can help with various tasks. Finally, invite them to ask questions or share ideas. Keep the tone positive and helpful. Let me structure that into a response.\\n'}, blocks=[TextBlock(block_type='text', text=\"Hello! I'm Qwen, a large language model developed by Alibaba Cloud. I can help you with a wide range of tasks, such as answering questions, creating content, and having conversations. How can I assist you today?\")])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e35a2ea-165d-4c35-8168-01295e72b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(message='What does Qwen mean?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c073535-1d35-4b2f-9d2f-1a87593cc4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Hi, What is your name ?')]),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': None, 'thinking': 'Okay, the user asked, \"Hi, What is your name?\" I need to respond appropriately. Let me think about how to approach this.\\n\\nFirst, the user is introducing themselves and asking for my name. Since I\\'m an AI, I should mention my name and maybe a bit about my purpose. But I should keep it friendly and concise.\\n\\nI should start with a greeting to match their \"Hi.\" Then state my name, which is Qwen. Next, explain that I\\'m a large language model developed by Alibaba Cloud. It\\'s good to mention my capabilities like answering questions, creating content, and having conversations. Also, include that I can help with various tasks. Finally, invite them to ask questions or share ideas. Keep the tone positive and helpful. Let me structure that into a response.\\n'}, blocks=[TextBlock(block_type='text', text=\"Hello! I'm Qwen, a large language model developed by Alibaba Cloud. I can help you with a wide range of tasks, such as answering questions, creating content, and having conversations. How can I assist you today?\")]),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='What does Qwen mean?')]),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': None, 'thinking': 'Okay, the user is asking what \"Qwen\" means. Let me recall the name\\'s origin. From what I remember, Qwen is derived from \"Question\" and \"Answer,\" which reflects the model\\'s purpose of answering questions and providing information. But I should check if there\\'s more to it. Also, maybe there\\'s a cultural or linguistic significance. The user might be interested in the etymology or the reasoning behind the name. I should explain it clearly, making sure to highlight the key aspects like the combination of question and answer, and perhaps mention that it\\'s a large language model developed by Alibaba Cloud. Need to keep it concise but informative. Let me structure the response: start by stating the meaning, explain the components, and relate it to the model\\'s function. Avoid any technical jargon. Make sure the user feels their question is fully addressed.\\n'}, blocks=[TextBlock(block_type='text', text='The name **Qwen** is derived from the combination of the words **\"Question\"** and **\"Answer\"**, reflecting the model\\'s core purpose of engaging in conversations, answering questions, and providing helpful information. It symbolizes the model\\'s ability to understand and respond to user inquiries effectively. Additionally, **Qwen** is a large language model developed by Alibaba Cloud, designed to assist with a wide range of tasks, from answering questions to creating content, coding, and more. Let me know if you\\'d like further details! ðŸ˜Š')])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "183c6d6c-8e45-4e87-b766-c23b7ec24c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Hi, What is your name ?\n",
      "assistant: Hello! I'm Qwen, a large language model developed by Alibaba Cloud. I can help you with a wide range of tasks, such as answering questions, creating content, and having conversations. How can I assist you today?\n",
      "user: What does Qwen mean?\n",
      "assistant: The name **Qwen** is derived from the combination of the words **\"Question\"** and **\"Answer\"**, reflecting the model's core purpose of engaging in conversations, answering questions, and providing helpful information. It symbolizes the model's ability to understand and respond to user inquiries effectively. Additionally, **Qwen** is a large language model developed by Alibaba Cloud, designed to assist with a wide range of tasks, from answering questions to creating content, coding, and more. Let me know if you'd like further details! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "for i in chat_engine.chat_history:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd0571bc-6ffe-4187-a108-3614654912ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = chat_engine.stream_chat(message='Can you tell me more about your developer?')\n",
    "dir(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b746e59e-93a9-428c-af87-25455e0a0ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen is developed by **Tongyi Lab**, a research team under **Alibaba Group**. Tongyi Lab is dedicated to advancing large-scale language models, AI technologies, and their applications. Alibaba Group, one of the world's leading technology companies, has a strong focus on innovation in artificial intelligence, cloud computing, and other cutting-edge technologies. \n",
      "\n",
      "Tongyi Lab's work on Qwen reflects Alibaba's commitment to creating powerful, versatile AI models that can assist users in diverse scenarios, from everyday tasks to complex problem-solving. If you'd like, I can share more details about Alibaba Group or Tongyi Lab's other projects! ðŸ˜Š"
     ]
    }
   ],
   "source": [
    "for i in response2.response_gen:\n",
    "    print(i, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83e22084-511e-4fdf-932f-9f525e588051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen is developed by **Tongyi Lab**, a research team under **Alibaba Group**. Tongyi Lab is dedicated to advancing large-scale language models, AI technologies, and their applications. Alibaba Group, one of the world's leading technology companies, has a strong focus on innovation in artificial intelligence, cloud computing, and other cutting-edge technologies. \n",
      "\n",
      "Tongyi Lab's work on Qwen reflects Alibaba's commitment to creating powerful, versatile AI models that can assist users in diverse scenarios, from everyday tasks to complex problem-solving. If you'd like, I can share more details about Alibaba Group or Tongyi Lab's other projects! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "for i in response2.response_gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ac6b558-884f-4491-95b4-8ee93ad0c0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_init__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_ensure_async_setup',\n",
       " 'achat_stream',\n",
       " 'aprint_response_stream',\n",
       " 'aput_in_queue',\n",
       " 'aqueue',\n",
       " 'async_response_gen',\n",
       " 'awrite_response_to_history',\n",
       " 'awrite_response_to_history_task',\n",
       " 'chat_stream',\n",
       " 'exception',\n",
       " 'is_done',\n",
       " 'is_function',\n",
       " 'is_function_false_event',\n",
       " 'is_function_not_none_thread_event',\n",
       " 'is_writing_to_memory',\n",
       " 'new_item_event',\n",
       " 'print_response_stream',\n",
       " 'put_in_queue',\n",
       " 'queue',\n",
       " 'response',\n",
       " 'response_gen',\n",
       " 'set_source_nodes',\n",
       " 'source_nodes',\n",
       " 'sources',\n",
       " 'unformatted_response',\n",
       " 'write_response_to_history']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f267b87-382b-4ccb-8563-ac67f4cb8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in response2.chat_stream:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97cda84c-c41e-40e4-a800-384d0d123412",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'StreamingAgentChatResponse' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m response2:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'StreamingAgentChatResponse' object is not iterable"
     ]
    }
   ],
   "source": [
    "for i in response2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30e56a48-0c2d-4e37-9aee-83fc6f51e1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_llm',\n",
       " '_memory',\n",
       " '_prefix_messages',\n",
       " 'achat',\n",
       " 'astream_chat',\n",
       " 'callback_manager',\n",
       " 'chat',\n",
       " 'chat_history',\n",
       " 'chat_repl',\n",
       " 'from_defaults',\n",
       " 'reset',\n",
       " 'stream_chat',\n",
       " 'streaming_chat_repl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chat_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e099f230-efe1-432f-8e28-1b4c94332a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_computed_fields__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_setattr_handlers__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__replace__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_recursion__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_estimate_token_count',\n",
       " '_format_memory_blocks',\n",
       " '_get_memory_blocks_content',\n",
       " '_get_value',\n",
       " '_insert_memory_content',\n",
       " '_iter',\n",
       " '_manage_queue',\n",
       " '_setattr_handler',\n",
       " '_truncate_memory_blocks',\n",
       " 'aget',\n",
       " 'aget_all',\n",
       " 'aput',\n",
       " 'aput_messages',\n",
       " 'areset',\n",
       " 'aset',\n",
       " 'audio_token_size_estimate',\n",
       " 'chat_history_token_ratio',\n",
       " 'class_name',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'custom_model_dump',\n",
       " 'dict',\n",
       " 'from_defaults',\n",
       " 'from_dict',\n",
       " 'from_json',\n",
       " 'from_orm',\n",
       " 'get',\n",
       " 'get_all',\n",
       " 'image_token_size_estimate',\n",
       " 'insert_method',\n",
       " 'json',\n",
       " 'memory_blocks',\n",
       " 'memory_blocks_template',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'put',\n",
       " 'put_messages',\n",
       " 'reset',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'session_id',\n",
       " 'set',\n",
       " 'sql_store',\n",
       " 'to_dict',\n",
       " 'to_json',\n",
       " 'token_flush_size',\n",
       " 'token_limit',\n",
       " 'tokenizer_fn',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_memory']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chat_engine._memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f746511-e1d8-4f0d-9b48-5e57d603784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_limit': 2000,\n",
       " 'token_flush_size': 1400,\n",
       " 'chat_history_token_ratio': 0.7,\n",
       " 'memory_blocks': [],\n",
       " 'memory_blocks_template': RichPromptTemplate(metadata={}, template_vars=['memory_blocks'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='\\n<memory>\\n{% for (block_name, block_content) in memory_blocks %}\\n<{{ block_name }}>\\n  {% for block in block_content %}\\n    {% if block.block_type == \"text\" %}\\n{{ block.text }}\\n    {% elif block.block_type == \"image\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | image }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | image }}\\n      {% endif %}\\n    {% elif block.block_type == \"audio\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | audio }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | audio }}\\n      {% endif %}\\n    {% endif %}\\n  {% endfor %}\\n</{{ block_name }}>\\n{% endfor %}\\n</memory>\\n'),\n",
       " 'insert_method': <InsertMethod.SYSTEM: 'system'>,\n",
       " 'image_token_size_estimate': 256,\n",
       " 'audio_token_size_estimate': 256,\n",
       " 'tokenizer_fn': functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'),\n",
       " 'sql_store': SQLAlchemyChatStore(table_name='llama_index_memory', async_database_uri='sqlite+aiosqlite:///:memory:'),\n",
       " 'session_id': '658e0d28-ec4f-4780-80b7-ad1510aff9ed'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine._memory.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "943e758a-25a7-48f5-abe4-d4735258a877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine._memory.memory_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebb5b80b-04ec-4daa-8654-c5cbb44a9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history =chat_engine._memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a601db1-92ec-4e49-84cc-e23dd16d5e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Hi, What is your name ?')]),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': None, 'thinking': 'Okay, the user asked, \"Hi, What is your name?\" I need to respond appropriately. Let me think about how to approach this.\\n\\nFirst, the user is introducing themselves and asking for my name. Since I\\'m an AI, I should mention my name and maybe a bit about my purpose. But I should keep it friendly and concise.\\n\\nI should start with a greeting to match their \"Hi.\" Then state my name, which is Qwen. Next, explain that I\\'m a large language model developed by Alibaba Cloud. It\\'s good to mention my capabilities like answering questions, creating content, and having conversations. Also, include that I can help with various tasks. Finally, invite them to ask questions or share ideas. Keep the tone positive and helpful. Let me structure that into a response.\\n'}, blocks=[TextBlock(block_type='text', text=\"Hello! I'm Qwen, a large language model developed by Alibaba Cloud. I can help you with a wide range of tasks, such as answering questions, creating content, and having conversations. How can I assist you today?\")]),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='What does Qwen mean?')]),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': None, 'thinking': 'Okay, the user is asking what \"Qwen\" means. Let me recall the name\\'s origin. From what I remember, Qwen is derived from \"Question\" and \"Answer,\" which reflects the model\\'s purpose of answering questions and providing information. But I should check if there\\'s more to it. Also, maybe there\\'s a cultural or linguistic significance. The user might be interested in the etymology or the reasoning behind the name. I should explain it clearly, making sure to highlight the key aspects like the combination of question and answer, and perhaps mention that it\\'s a large language model developed by Alibaba Cloud. Need to keep it concise but informative. Let me structure the response: start by stating the meaning, explain the components, and relate it to the model\\'s function. Avoid any technical jargon. Make sure the user feels their question is fully addressed.\\n'}, blocks=[TextBlock(block_type='text', text='The name **Qwen** is derived from the combination of the words **\"Question\"** and **\"Answer\"**, reflecting the model\\'s core purpose of engaging in conversations, answering questions, and providing helpful information. It symbolizes the model\\'s ability to understand and respond to user inquiries effectively. Additionally, **Qwen** is a large language model developed by Alibaba Cloud, designed to assist with a wide range of tasks, from answering questions to creating content, coding, and more. Let me know if you\\'d like further details! ðŸ˜Š')]),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Can you tell me more about your developer?')]),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': [], 'thinking': \"Okay, the user asked about the developer of Qwen. I need to provide information about Alibaba Cloud and Tongyi Lab.\\n\\nFirst, I should mention that Alibaba Cloud is part of Alibaba Group and is a leading cloud computing company. Then, introduce Tongyi Lab as the research team behind Qwen. Highlight their focus areas like natural language processing and AI.\\n\\nAlso, mention the broader context of Alibaba Group's involvement in AI and technology. Make sure to keep the information clear and concise, avoiding any technical jargon. Check if there are any key points I might have missed, like the timeline or other projects by the lab. Ensure the response is friendly and invites further questions.\\n\"}, blocks=[TextBlock(block_type='text', text=\"Qwen is developed by **Tongyi Lab**, a research team under **Alibaba Group**. Tongyi Lab is dedicated to advancing large-scale language models, AI technologies, and their applications. Alibaba Group, one of the world's leading technology companies, has a strong focus on innovation in artificial intelligence, cloud computing, and other cutting-edge technologies. \\n\\nTongyi Lab's work on Qwen reflects Alibaba's commitment to creating powerful, versatile AI models that can assist users in diverse scenarios, from everyday tasks to complex problem-solving. If you'd like, I can share more details about Alibaba Group or Tongyi Lab's other projects! ðŸ˜Š\")])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7da186f2-a5eb-4df0-af34-022b69eacf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm Qwen, a large language model developed by Alibaba Cloud. I can help you with a wide range of tasks, such as answering questions, creating content, and having conversations. How can I assist you today?\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history[1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e23eb9f-f52f-4fc0-9386-43a61d299296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf5bcc8a-3a50-4b20-ad19-0cf790a2b8e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CondenseQuestionChatEngine.__init__() missing 4 required positional arguments: 'query_engine', 'condense_question_prompt', 'memory', and 'llm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CondenseQuestionChatEngine\n\u001b[0;32m----> 3\u001b[0m condense_chat_engine \u001b[38;5;241m=\u001b[39m \u001b[43mCondenseQuestionChatEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: CondenseQuestionChatEngine.__init__() missing 4 required positional arguments: 'query_engine', 'condense_question_prompt', 'memory', and 'llm'"
     ]
    }
   ],
   "source": [
    "condense_chat_engine = CondenseQuestionChatEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85b9e651-3347-40e2-a296-c78a30baa54b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CondenseQuestionChatEngine.from_defaults() missing 1 required positional argument: 'query_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m memory \u001b[38;5;241m=\u001b[39m Memory\u001b[38;5;241m.\u001b[39mfrom_defaults(token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m condense_chat_engine \u001b[38;5;241m=\u001b[39m \u001b[43mCondenseQuestionChatEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: CondenseQuestionChatEngine.from_defaults() missing 1 required positional argument: 'query_engine'"
     ]
    }
   ],
   "source": [
    "memory = Memory.from_defaults(token_limit=500)\n",
    "condense_chat_engine = CondenseQuestionChatEngine.from_defaults(llm=llm, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40e3b3b8-b5a3-4547-bc8d-9e67af59da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24349569-9107-48ad-84c5-5e4bd86ac128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_documents in module llama_index.core.indices.base:\n",
      "\n",
      "from_documents(documents: Sequence[llama_index.core.schema.Document], storage_context: Optional[llama_index.core.storage.storage_context.StorageContext] = None, show_progress: bool = False, callback_manager: Optional[llama_index.core.callbacks.base.CallbackManager] = None, transformations: Optional[List[llama_index.core.schema.TransformComponent]] = None, **kwargs: Any) -> ~IndexType method of abc.ABCMeta instance\n",
      "    Create index from documents.\n",
      "    \n",
      "    Args:\n",
      "        documents (Sequence[Document]]): List of documents to\n",
      "            build the index from.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(VectorStoreIndex.from_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb8dc67f-07bc-4862-b6b0-0bcca74dcb7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`llama-index-embeddings-openai` package not found, please run `pip install llama-index-embeddings-openai`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-ui-hFrWzTN_-py3.10/lib/python3.10/site-packages/llama_index/core/embeddings/utils.py:50\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     51\u001b[0m         OpenAIEmbedding,\n\u001b[1;32m     52\u001b[0m     )  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     55\u001b[0m         validate_openai_api_key,\n\u001b[1;32m     56\u001b[0m     )  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.embeddings'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Empty index (zero documents)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-ui-hFrWzTN_-py3.10/lib/python3.10/site-packages/llama_index/core/indices/base.py:122\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    115\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    116\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     transformations,\n\u001b[1;32m    118\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    120\u001b[0m )\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-ui-hFrWzTN_-py3.10/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:73\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m     79\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     86\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-ui-hFrWzTN_-py3.10/lib/python3.10/site-packages/llama_index/core/settings.py:64\u001b[0m, in \u001b[0;36m_Settings.embed_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the embedding model.\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chat-bot-ui-hFrWzTN_-py3.10/lib/python3.10/site-packages/llama_index/core/embeddings/utils.py:61\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     59\u001b[0m     validate_openai_api_key(embed_model\u001b[38;5;241m.\u001b[39mapi_key)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-embeddings-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-embeddings-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI embedding model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: `llama-index-embeddings-openai` package not found, please run `pip install llama-index-embeddings-openai`"
     ]
    }
   ],
   "source": [
    "# Empty index (zero documents)\n",
    "documents = []\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67622679-c504-4367-9021-a81f4b6a7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87bde7b9-a721-483a-bc01-ec8383317c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model (using HuggingFace for offline/local embedding)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91c645d4-06f4-446e-b85d-bff361ecaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Index with embedding model specified\n",
    "index = VectorStoreIndex.from_documents([], embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c4cbf92-ba27-401c-85d2-4a31a0011f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d615c731-58a9-4508-bb63-9a7a88c42f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(llm=llm, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5d7911bb-53f2-4d9b-ba69-d06c76a6f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e0884c66-9d65-42c1-9a1c-09eb44d1cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92e025-4a18-4733-9803-13069ec51cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1897a3c7-dae6-45e9-a39d-c3d817c5eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TEMPLATE = \"\"\"\\\n",
    "Given a conversation (between Human and Assistant) and a follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<Follow Up Message>\n",
    "{question}\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_PROMPT = PromptTemplate(DEFAULT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "70c3751b-f27e-4cc3-9a19-90e47537b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = CondenseQuestionChatEngine(\n",
    "                query_engine=query_engine, \n",
    "                condense_question_prompt = DEFAULT_PROMPT,\n",
    "                memory = memory, llm = llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1426d-03b1-43ac-99fd-474f461ec1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ec88931d-b72a-4423-8528-5a26a0d9a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "38e58722-d9d5-413f-aa6f-3144f871b88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Empty Response', sources=[ToolOutput(content='Empty Response', tool_name='query_engine', raw_input={'query': 'What is the capital of France?'}, raw_output=Response(response='Empty Response', source_nodes=[], metadata=None), is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "08cfa8ca-0bba-457c-b1af-902f99d18a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMemoryBuffer(chat_store=SimpleChatStore(store={'chat_history': [ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='What is the capital of France?')]), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Empty Response')])]}), chat_store_key='chat_history', token_limit=500, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine._memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9358c815-9873-4023-86e6-ef1c9a609e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: What is the capital of France?\n",
      "assistant: Empty Response\n"
     ]
    }
   ],
   "source": [
    "for i in chat_engine.chat_history:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
