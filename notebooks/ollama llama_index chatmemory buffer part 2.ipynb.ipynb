{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713946f3-3f9b-4368-901b-bd07a758bfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b61210-d139-4cc2-858a-0c38c960b669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/chat-bot-ui-hFrWzTN_-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatSummaryMemoryBuffer\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from transformers import AutoTokenizer\n",
    "#from llama_index.llms.openai import OpenAI as OpenAiLlm\n",
    "#import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132135b-c60d-4726-be42-cffae283929f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4597764d-c625-42af-8589-34d4dcd7d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    ChatMessage(role=\"user\", content=\"What is LlamaIndex?\"),\n",
    "    ChatMessage(\n",
    "        role=\"assistant\",\n",
    "        content=\"LlamaaIndex is the leading data framework for building LLM applications\",\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Can you give me some more details?\"),\n",
    "    ChatMessage(\n",
    "        role=\"assistant\",\n",
    "        content=\"\"\"LlamaIndex is a framework for building context-augmented LLM applications. Context augmentation refers to any use case that applies LLMs on top of your private or domain-specific data. Some popular use cases include the following: \n",
    "        Question-Answering Chatbots (commonly referred to as RAG systems, which stands for \"Retrieval-Augmented Generation\"), Document Understanding and Extraction, Autonomous Agents that can perform research and take actions\n",
    "        LlamaIndex provides the tools to build any of these above use cases from prototype to production. The tools allow you to both ingest/process this data and implement complex query workflows combining data access with LLM prompting.\"\"\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7358410-4886-45a1-b262-42872709ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"qwen3:14b\"\n",
    "model = 'qwen3:14b'\n",
    "tokenizer_model = \"Qwen/Qwen3-14B\"\n",
    "context_window = 1000\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "\n",
    "llm = Ollama(\n",
    "    model=model,\n",
    "    request_timeout=120.0,\n",
    "    thinking=False,\n",
    "    context_window=context_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d0124a-b6a3-4882-ab5e-3b9ba02cc429",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_llm = llm\n",
    "#tokenizer_fn = tiktoken.encoding_for_model(model).encode\n",
    "tokenizer_fn = lambda text: tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "memory = ChatSummaryMemoryBuffer.from_defaults(\n",
    "    chat_history=chat_history,\n",
    "    llm=summarizer_llm,\n",
    "    token_limit=2,\n",
    "    tokenizer_fn=tokenizer_fn,\n",
    ")\n",
    "\n",
    "history = memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d06a7f-bc07-41cf-b8ce-c79c68d03095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='The conversation discusses LlamaIndex, a framework for building context-augmented LLM applications. It explains that LlamaIndex enables the use of large language models (LLMs) with private or domain-specific data, supporting use cases like question-answering chatbots (RAG systems), document understanding, and autonomous agents. The framework provides tools for data ingestion, processing, and complex query workflows that combine data access with LLM prompting.')])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5419df-5cd6-43b7-abad-4f62ad84f89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_computed_fields__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_setattr_handlers__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__replace__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_recursion__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_get_prompt_to_summarize',\n",
       " '_get_value',\n",
       " '_handle_assistant_and_tool_messages',\n",
       " '_iter',\n",
       " '_setattr_handler',\n",
       " '_split_messages_summary_or_full_text',\n",
       " '_summarize_oldest_chat_history',\n",
       " '_token_count_for_messages',\n",
       " 'aget',\n",
       " 'aget_all',\n",
       " 'aput',\n",
       " 'aput_messages',\n",
       " 'areset',\n",
       " 'aset',\n",
       " 'chat_store',\n",
       " 'chat_store_key',\n",
       " 'class_name',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'count_initial_tokens',\n",
       " 'custom_model_dump',\n",
       " 'dict',\n",
       " 'from_defaults',\n",
       " 'from_dict',\n",
       " 'from_json',\n",
       " 'from_orm',\n",
       " 'from_string',\n",
       " 'get',\n",
       " 'get_all',\n",
       " 'get_token_count',\n",
       " 'json',\n",
       " 'llm',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'put',\n",
       " 'put_messages',\n",
       " 'reset',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'serialize_courses_in_order',\n",
       " 'set',\n",
       " 'summarize_prompt',\n",
       " 'to_dict',\n",
       " 'to_json',\n",
       " 'to_string',\n",
       " 'token_limit',\n",
       " 'tokenizer_fn',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_memory']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebad272-6e01-43a3-9b63-6068a4f41e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following is a conversation between the user and assistant. Write a concise summary about the contents of this conversation.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.summarize_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f03d4cf5-eae1-4358-9af9-c3e4c67084e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chat_history = [\n",
    "    ChatMessage(role=\"user\", content=\"Why context augmentation?\"),\n",
    "    ChatMessage(\n",
    "        role=\"assistant\",\n",
    "        content=\"LLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data. However, they are not trained on your data, which may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks. LlamaIndex provides tooling to enable context augmentation. A popular example is Retrieval-Augmented Generation (RAG) which combines context with LLMs at inference time. Another is finetuning.\",\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Who is LlamaIndex for?\"),\n",
    "    ChatMessage(\n",
    "        role=\"assistant\",\n",
    "        content=\"LlamaIndex provides tools for beginners, advanced users, and everyone in between. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. For more complex applications, our lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, reranking modules—to fit their needs.\",\n",
    "    ),\n",
    "]\n",
    "memory.put(new_chat_history[0])\n",
    "memory.put(new_chat_history[1])\n",
    "memory.put(new_chat_history[2])\n",
    "memory.put(new_chat_history[3])\n",
    "history = memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5e2b7b9-1673-4ed7-a1b0-232f9ef2e2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='The conversation discusses LlamaIndex, a framework that enables the integration of large language models (LLMs) with private or domain-specific data through context augmentation. It explains the importance of context augmentation to address the limitations of pre-trained LLMs, which are not trained on user-specific data. The framework supports various use cases, such as RAG systems and autonomous agents, and provides tools for data ingestion, processing, and querying. LlamaIndex is designed for users of all skill levels, offering both high-level and low-level APIs to accommodate different needs.')])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c564569-275c-4cdf-aed2-43ac61b9675f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4becfa0c-9899-47e5-8aef-4e631e62369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='The conversation discusses LlamaIndex, a framework for building context-augmented LLM applications. It explains that LlamaIndex enables the use of large language models (LLMs) in conjunction with private or domain-specific data, with use cases such as question-answering chatbots (RAG systems), document understanding, and autonomous agents. The framework provides tools for data ingestion, processing, and implementing complex query workflows that combine data access with LLM prompting.')]), ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Why context augmentation?')]), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text=\"LLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data. However, they are not trained on your data, which may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks. LlamaIndex provides tooling to enable context augmentation. A popular example is Retrieval-Augmented Generation (RAG) which combines context with LLMs at inference time. Another is finetuning.\")]), ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Who is LlamaIndex for?')]), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='LlamaIndex provides tools for beginners, advanced users, and everyone in between. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. For more complex applications, our lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, reranking modules—to fit their needs.')])]\n"
     ]
    }
   ],
   "source": [
    "memory = ChatSummaryMemoryBuffer.from_defaults(\n",
    "    chat_history=chat_history + new_chat_history,\n",
    "    llm=summarizer_llm,\n",
    "    token_limit=256,\n",
    "    tokenizer_fn=tokenizer_fn,\n",
    ")\n",
    "print(memory.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caed5c47-a882-4175-9fd8-3bde4872e830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "937300e7-fb16-47fc-846e-9e766514dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: The conversation discusses LlamaIndex, a framework for building context-augmented LLM applications. It explains that LlamaIndex enables the use of large language models (LLMs) in conjunction with private or domain-specific data, with use cases such as question-answering chatbots (RAG systems), document understanding, and autonomous agents. The framework provides tools for data ingestion, processing, and implementing complex query workflows that combine data access with LLM prompting.\n",
      "=====\n",
      "user: Why context augmentation?\n",
      "=====\n",
      "assistant: LLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data. However, they are not trained on your data, which may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks. LlamaIndex provides tooling to enable context augmentation. A popular example is Retrieval-Augmented Generation (RAG) which combines context with LLMs at inference time. Another is finetuning.\n",
      "=====\n",
      "user: Who is LlamaIndex for?\n",
      "=====\n",
      "assistant: LlamaIndex provides tools for beginners, advanced users, and everyone in between. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. For more complex applications, our lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, reranking modules—to fit their needs.\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in memory.get():\n",
    "    print(i)\n",
    "    print(5*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa1847-3c87-48b8-b2e5-1116c7ea9544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d0a90-efdb-4678-a61e-3cafa780b6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
